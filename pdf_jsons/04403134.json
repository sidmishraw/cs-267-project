{
  "Page#1": [
    "A Weighted Consensus Function Based on Information-theoretic Principles to",
    "Combine “Soft” Clusterings",
    "",
    "Yan Gao",
    "1",
    "Shiwen Gu",
    "1",
    "Jianhua Li",
    "1",
    "Zhining Liao",
    "2",
    "",
    "1",
    "School of Information Science and Engineeri",
    "ng, Central South University University",
    "410083, Hunan, P.R.China China",
    "gaoyan,swgu,jhli}@csu.edu.cn",
    "2",
    "Department of Computer Science, Loughborough University",
    "LE113TU, UK",
    "liaozn@yahoo.com",
    "",
    "",
    "Abstract",
    "",
    "How to combine multiple clusterings into a single",
    "clustering solution of better quality is a critical",
    "problem in cluster ensemble. In this paper, we extend",
    "strehl’s consensus function based on informationtheoretic principles and  propose a novel weighted",
    "consensus function to combine multiple “soft",
    "clusterings. In our consensus function, we use mutual",
    "information to measure the sharing information",
    "between two “soft” clusterings and emphasize the",
    "clustering which is much different from the others. We",
    "use the algorithm similar to sequential k-means to",
    "obtain the solution of this consensus function and",
    "conduct experiments on four real-world datasets to",
    "compare our algorithm with other four consensus",
    "function, including CSPA, HGPA, MCLA, QMI. The",
    "results indicate that our consensus function provides",
    "solutions of better quality than CSPA, HGPA, MCLA",
    "QMI and when the distribution of diversity in cluster",
    "ensembles is uneven, considering the influence of",
    "diversity can improve the quality of clustering",
    "ensemble",
    "",
    "1. Introduction",
    "",
    "Combining the results of several clustering methods",
    "has recently appeared as one of the branches of",
    "multiple classifier systems. The aim of combining",
    "clustering results is to improve the quality and",
    "robustness of the results. The current clustering",
    "ensemble literature has mainly focused on two main",
    "problems: how to combine multiple clusterings",
    "commonly referred to as the problem of consensus",
    "function\\; how to produce multiple clustering results",
    "and which diversity they have",
    "There are several efficient consensus functions",
    "derived from statistical, graph-based and informationtheoretic principles etc. Fred & Jain[1",
    "",
    "F",
    "e",
    "r",
    "n",
    "",
    "Brodley[2",
    "",
    "M",
    "o",
    "n",
    "t",
    "i",
    "et al",
    "",
    "3",
    "",
    "es",
    "t",
    "a",
    "b",
    "l",
    "i",
    "s",
    "h",
    "e",
    "d",
    "th",
    "e co",
    "association matrix based on similarities between",
    "different clustering solutions, and then use",
    "agglomerative hierarchical clustering. Topchy et al",
    "4",
    "",
    "5",
    "",
    "pr",
    "op",
    "os",
    "e",
    "d",
    "a",
    "m",
    "i",
    "x",
    "t",
    "ur",
    "e",
    "m",
    "o",
    "d",
    "e",
    "l",
    "i",
    "n",
    "or",
    "d",
    "e",
    "r",
    "",
    "t",
    "o",
    "obt",
    "a",
    "i",
    "n",
    "a",
    "",
    "consensus function. They also established a quadratic",
    "mutual information criterion for clustering ensemble",
    "and the approximate results for this criterion can be",
    "obtained by running k-means",
    "",
    "W",
    "",
    "",
    "T",
    "a",
    "n",
    "g",
    "",
    "a",
    "n",
    "d",
    "",
    "Z.H.Zhou[6",
    "pr",
    "op",
    "os",
    "e",
    "d",
    "ba",
    "g",
    "g",
    "i",
    "n",
    "g",
    "b",
    "a",
    "s",
    "e",
    "d",
    "s",
    "e",
    "l",
    "e",
    "c",
    "t",
    "i",
    "v",
    "e",
    "c",
    "l",
    "u",
    "s",
    "t",
    "e",
    "r",
    "",
    "ensemble algorithm in which the normalized mutual",
    "information between the clustering result and other",
    "results can be regarded as the weight of clustering",
    "result in bagging. Frossyniotis[7",
    "",
    "ap",
    "p",
    "lie",
    "d",
    "b",
    "o",
    "o",
    "s",
    "tin",
    "g",
    "to",
    "",
    "clustering ensemble. Strehl & Ghosh[8",
    "pr",
    "op",
    "os",
    "e",
    "d",
    "t",
    "h",
    "r",
    "e",
    "e",
    "",
    "different approaches to generat consensus functions",
    "which are all based on hypergraph partitioning. They",
    "also point out that clustering ensemble can be regarded",
    "as the optimal problem based on mutual information",
    "but not point out how to solve it and it is only to",
    "combine “hard” clustering",
    "Diversity plays the same significant role in cluster",
    "ensembles as classifier ensembles. Fern and Brodley",
    "9",
    "",
    "n",
    "o",
    "te",
    "d",
    "t",
    "h",
    "at",
    "",
    "mo",
    "r",
    "e",
    "d",
    "i",
    "v",
    "e",
    "r",
    "s",
    "e",
    "e",
    "n",
    "s",
    "e",
    "mb",
    "le",
    "s",
    "o",
    "f",
    "f",
    "e",
    "r",
    "e",
    "d",
    "la",
    "r",
    "g",
    "e",
    "r",
    "",
    "improvement on the individual accuracy than less",
    "diverse ensembles. Kuncheva",
    "a",
    "l",
    "s",
    "o",
    "s",
    "t",
    "ud",
    "i",
    "e",
    "d",
    "t",
    "h",
    "e",
    "",
    "diversity within such cluster ensembles and proposed a",
    "variant of the generic pairwise cluster ensemble",
    "approach which enforces diversity in the ensemble",
    "In this paper, we extend Strehl’s consensus function",
    "based on information-theoretic principles and propose",
    "a weighted consensus function to combine “soft” and",
    "hard” clusterings. In our consensus function, we use",
    "mutual information to measure the sharing information",
    "2007 IEEE International Conference on Granular Computing",
    "0-7695-3032-X/07 $25.00 © 2007 IEEE",
    "DOI 10.1109/GrC.2007.156",
    "417",
    "2007 IEEE International Conference on Granular Computing",
    "0-7695-3032-X/07 $25.00 © 2007 IEEE",
    "DOI 10.1109/GrC.2007.156",
    "417"
  ],
  "Page#2": [
    "between two “soft” clusterings and emphasize the",
    "clustering which is much different from the others",
    "The paper is organized as follows. Section 2",
    "discusses related work: Strehl’s objective function for",
    "clustering ensemble and the measurement for diversity",
    "in cluster ensembles. Section 3 discusses a weighted",
    "consensus function based on information-theoretical",
    "principles to combine “soft” clusterings and the",
    "algorithm to implement consensus function. We",
    "conduct experiments in Section 4 and conclude with",
    "Section 5",
    "",
    "2.Related work",
    "",
    "2.1 Consensus Function Based on Mutual",
    "Information",
    "",
    "Strehl and Ghosh thought that combined clustering",
    "should share the most information with the original",
    "clusterings",
    "C",
    "1",
    "C",
    "r",
    "But how do we measure",
    "shared information between clusterings? Strehl and",
    "Ghosh used normalized mutual information to measure",
    "it",
    "Suppose there are two clusterings: c",
    "a",
    "and c",
    "b",
    "Let n",
    "h",
    "is",
    "the number of instances which label is c",
    "l",
    "in clustering",
    "C",
    "b",
    "n",
    "l",
    "is the number of instance which label is c",
    "h",
    "in",
    "clustering C",
    "a",
    "n",
    "l",
    "h",
    "is the number of instance which label",
    "not only is c",
    "l",
    "in clustering C",
    "b",
    "but also is c",
    "h",
    "in clustering",
    "C",
    "a",
    "n is the total number of instances in data set. The",
    "normalize mutual information between C",
    "a",
    "and C",
    "b",
    "is",
    "",
    "",
    "",
    "K",
    "k",
    "K",
    "k",
    "l",
    "h",
    "h",
    "l",
    "k",
    "h",
    "l",
    "NMI",
    "n",
    "n",
    "n",
    "n",
    "n",
    "n",
    "C",
    "C",
    "b",
    "a",
    "11",
    "",
    "",
    "log",
    "2",
    "",
    "",
    "",
    "2",
    "",
    "",
    "",
    "1",
    "Strehl’s objective function is defined as",
    "",
    "",
    "r",
    "q",
    "NMI",
    "C",
    "opt",
    "q",
    "C",
    "C",
    "C",
    "1",
    "",
    "",
    "",
    "max",
    "arg",
    "",
    "",
    "",
    "2",
    "Although Strehl&Ghosh proposed the objective",
    "function based on mutual information for clustering",
    "ensemble, they pointed out that for finite populations",
    "the trivial solution is to exhaustively search through all",
    "possible clusterings with k labels \\(approximately kn/k",
    "for n>> k\\ for the one with the maximum ANMI which",
    "is computationally prohibitive. And this objective",
    "function is only applied to combine “hard” clusterings",
    "A.Topchy[5",
    "als",
    "o",
    "e",
    "s",
    "tab",
    "lis",
    "h",
    "e",
    "d",
    "t",
    "h",
    "e",
    "o",
    "b",
    "j",
    "ec",
    "tiv",
    "e",
    "",
    "f",
    "u",
    "n",
    "c",
    "tio",
    "n",
    "",
    "based on mutual information similar to",
    "Strehl&Ghosh’s. But they used quadratic mutual",
    "information. They proved that the approximate",
    "solution for their consensus function could be obtained",
    "by using k-means on the new feature space that is built",
    "on the original clusterings. And in Strehl’s objective",
    "function, the normalized mutual information is used to",
    "measure shared information between two “hard",
    "clusterings, So their algorithm is also only applied to",
    "combine “hard” clusterings",
    "",
    "",
    "2.2",
    "",
    "Diversity Between a Pair of Clusterings",
    "",
    "",
    "Diversity within an ensemble is of vital importance",
    "for its success. There are many methods to measure the",
    "diversity between a pair of clusterings: \\(1\\ir",
    "counting[1",
    "",
    "13][14",
    "",
    "",
    "",
    "2",
    "",
    "e",
    "t",
    "m",
    "a",
    "t",
    "c",
    "h",
    "i",
    "ng [15",
    "",
    "a",
    "nd \\(3",
    "",
    "variation of information \\(VI",
    "",
    "",
    "",
    "T",
    "h",
    "e",
    "p",
    "a",
    "i",
    "r",
    "",
    "counting method evaluates the similarity between two",
    "clustering algorithms by examining how likely they are",
    "to group a pair of objects together, or separate them in",
    "different clusters. All pair counting methods are",
    "restricted to handling hard clusterings. Other",
    "drawbacks of pair counting methods are also discussed",
    "in [12",
    "",
    "",
    "Th",
    "e s",
    "e",
    "t ma",
    "t",
    "c",
    "h",
    "i",
    "n",
    "g",
    "",
    "m",
    "e",
    "th",
    "o",
    "d",
    "s",
    "eek",
    "s",
    "f",
    "o",
    "r",
    "a",
    "m",
    "a",
    "t",
    "c",
    "h",
    "",
    "between clusters, that is, the sets of objects grouped",
    "together in two clusterings respectively. Existing set",
    "matching approaches perform matching in a stepwise",
    "manner without a global optimization objective",
    "VI is the criterion based on information-theoretic",
    "principles for comparing two clusterings on the same",
    "dataset. The criterion makes no assumptions about how",
    "the clusterings were generated and can be applied to",
    "compare both soft and hard clusterings. Supposing C",
    "and C’ are two clusterings, the value of VI between",
    "clustering C and C’ is computed as follows",
    "",
    "",
    "",
    "",
    "",
    "2",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "C",
    "C",
    "I",
    "C",
    "H",
    "C",
    "H",
    "C",
    "C",
    "VI",
    "Š",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "C",
    "C",
    "H",
    "C",
    "C",
    "H",
    "",
    "",
    "",
    "",
    "3",
    "Where H\\(C|C’\\ ,H\\(C’|C\\ are both the conditional",
    "entropies. H\\(C|C’\\ measures the amount of information",
    "about C that we loose, while H\\(C’|C\\ measures the",
    "amount of information about C’ that we still have to",
    "gain, when going from clustering C to clustering C’. If",
    "C=C’, VI \\(C, C’\\ =0. If VI \\(C, C’\\>VI\\(C,C’’\\, the C is",
    "more different form C’ than C",
    "Before computing VI of two clusterings, we should",
    "know the joint distribution of two clusterings. If C and",
    "C’ are both hard clusterings, c is the cluster belonging",
    "to C, c’ is the cluster belonging to C’, n is the number",
    "of instance on the dataset X, then the joint distribution",
    "p\\(c, c’\\ used in VI is computed as follows",
    "",
    "n",
    "c",
    "c",
    "c",
    "c",
    "p",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "4",
    "If C and C’ are both soft clusterings, c is the cluster",
    "belonging to C, c’ is the cluster belonging to C",
    "supposing the knowing cluster c is irrelevant to c’ if",
    "418",
    "418"
  ],
  "Page#3": [
    "the chosen instance x is known, i.e. p\\(c’|x,c\\=p\\(c’|x",
    "the joint distribution P\\(c, c’\\ is computed as follows",
    "",
    "x",
    "x",
    "p",
    "x",
    "c",
    "p",
    "x",
    "c",
    "p",
    "c",
    "c",
    "p",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "5",
    "",
    "",
    "3. The weighted consensus function",
    "",
    "3.1 The weighted objective function based on",
    "mutual information",
    "",
    "We also think that cluster ensemble should extract",
    "the combined clustering sharing the most information",
    "with the original clusterings. But in this paper",
    "considering “soft” clusterings and the influence of",
    "diversity in cluster ensembles, we propose a weighted",
    "objective function for cluster ensemble",
    "First, instead of normalized mutual information, we",
    "use the Shannon mutual information to measure shared",
    "information between two “soft” clusterings",
    "Suppose there are a set of clusterings on dataset X",
    "C",
    "1",
    "C",
    "r",
    "where C",
    "q",
    "is the “soft” clustering",
    "C",
    "q",
    "c",
    "q",
    "1",
    "c",
    "q",
    "k",
    "C is the combined clustering",
    "Supposing C",
    "q",
    "be any clustering in",
    "the information",
    "of the cluster c",
    "q",
    "j",
    "in the clustering C",
    "q",
    "should be",
    "transferred to cluster c",
    "i",
    "in clustering C \\(c",
    "q",
    "j",
    "x-> c",
    "i",
    "So",
    "the joint probability p\\(c",
    "q",
    "j",
    "c",
    "i",
    "can be assumed as",
    "follows",
    "",
    "x",
    "x",
    "p",
    "x",
    "c",
    "p",
    "x",
    "c",
    "p",
    "c",
    "c",
    "p",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "6",
    "The marginal probability p\\(c",
    "q",
    "j",
    "is",
    "",
    "x",
    "x",
    "p",
    "x",
    "c",
    "p",
    "c",
    "p",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "j",
    "q",
    "j",
    "q",
    "",
    "7",
    "The marginal probability p\\(c",
    "i",
    "is",
    "",
    "",
    "x",
    "i",
    "i",
    "x",
    "p",
    "x",
    "c",
    "p",
    "c",
    "p",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "8",
    "Consequently the Shannon mutual information",
    "between “soft” clustering C",
    "q",
    "and the combined",
    "clustering C can be computed as follows",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "log",
    "",
    "",
    "",
    "",
    "",
    "",
    "11",
    "i",
    "j",
    "q",
    "i",
    "j",
    "q",
    "k",
    "i",
    "k",
    "j",
    "i",
    "j",
    "q",
    "q",
    "c",
    "p",
    "c",
    "p",
    "c",
    "c",
    "p",
    "c",
    "c",
    "p",
    "C",
    "C",
    "I",
    "",
    "",
    "",
    "",
    "9",
    "Second, we consider the influence of diversity",
    "within an ensemble in our objective function. We use",
    "VI to measure the difference between two “soft” or",
    "hard” clusterings. The diversity of every clustering is",
    "defined as the average of VI between itself and other",
    "clusterings. So the diversity can be computed as",
    "follows",
    "",
    "",
    "",
    "Š",
    "",
    "i",
    "j",
    "1..r",
    "j",
    "j",
    "i",
    "",
    "",
    "VI",
    "1",
    "r",
    "1",
    "",
    "i",
    "",
    "C",
    "C",
    "div",
    "",
    "",
    "10",
    "The clustering with high value of diversity is more",
    "different from other clusterings. It contains much",
    "information which other clusterings don’t have. We",
    "think this clustering is important in clustering",
    "ensemble",
    "Considering “soft” clusterings and the influence of",
    "diversity in cluster ensembles, the weighted objective",
    "function to combine “soft” clusterings can be defined",
    "as",
    "",
    "",
    "r",
    "q",
    "opt",
    "q",
    "I",
    "q",
    "1",
    "C",
    "",
    "C",
    "",
    "C",
    "",
    "",
    "",
    "div",
    "max",
    "arg",
    "C",
    "",
    "",
    "11",
    "",
    "3.2 Algorithm implementation",
    "",
    "In this section we propose an algorithm to solve the",
    "weighted objective function mentioned in section 3.1",
    "First we define F",
    "q=1..r",
    "C",
    "q",
    "C\\. F is",
    "decomposable, i.e. if the combined clustering is",
    "C={c",
    "1",
    "c",
    "k",
    "then F",
    "i",
    "F\\(c",
    "i",
    "",
    "",
    "F\\(c",
    "i",
    "",
    "q=1..r",
    "div\\(q",
    "I\\(c",
    "i",
    "C",
    "q",
    "So when the instance x is merged into the",
    "cluster c",
    "i",
    "the value of F is decreased and the change is",
    "computed as follows",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "x",
    "c",
    "F",
    "x",
    "F",
    "c",
    "F",
    "x",
    "c",
    "d",
    "i",
    "i",
    "i",
    "F",
    "Š",
    "",
    "",
    "",
    "0",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "C",
    "",
    "",
    "",
    "",
    "",
    "",
    "C",
    "",
    "",
    "",
    "",
    "",
    "C",
    "",
    "",
    "",
    "",
    "11",
    "1",
    "r",
    "1",
    "q",
    "r",
    "1",
    "q",
    "",
    "",
    "",
    "Š",
    "",
    "",
    "",
    "",
    "",
    "",
    "i",
    "j",
    "q",
    "j",
    "q",
    "i",
    "r",
    "q",
    "k",
    "j",
    "r",
    "q",
    "i",
    "i",
    "c",
    "c",
    "p",
    "x",
    "c",
    "p",
    "JS",
    "c",
    "p",
    "x",
    "p",
    "q",
    "div",
    "x",
    "c",
    "I",
    "q",
    "div",
    "x",
    "I",
    "q",
    "div",
    "c",
    "I",
    "q",
    "div",
    "q",
    "q",
    "q",
    "",
    "12",
    "Where C",
    "q",
    "is “soft” clustering, c",
    "q",
    "j",
    "is the cluster in C",
    "q",
    "",
    "JS is the Jensen-Shannon divergence defined as",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "2",
    "1",
    "p",
    "c",
    "c",
    "p",
    "D",
    "p",
    "x",
    "c",
    "p",
    "D",
    "c",
    "c",
    "p",
    "x",
    "c",
    "p",
    "JS",
    "i",
    "j",
    "q",
    "KL",
    "j",
    "q",
    "KL",
    "i",
    "j",
    "q",
    "j",
    "q",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "13",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "2",
    "1",
    "i",
    "i",
    "i",
    "c",
    "p",
    "x",
    "p",
    "c",
    "p",
    "c",
    "p",
    "x",
    "p",
    "x",
    "p",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "2",
    "1",
    "i",
    "j",
    "q",
    "j",
    "q",
    "c",
    "c",
    "p",
    "x",
    "c",
    "p",
    "p",
    "",
    "",
    "",
    "",
    "",
    "",
    "If x is removed from its current cluster c",
    "j",
    "to c",
    "i",
    "the",
    "change of F is computed as follows",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "x",
    "c",
    "d",
    "x",
    "c",
    "d",
    "c",
    "F",
    "x",
    "c",
    "F",
    "F",
    "i",
    "F",
    "j",
    "F",
    "j",
    "i",
    "Š",
    "",
    "Š",
    "",
    "",
    "",
    "14",
    "Where c",
    "j",
    "is the cluster which is formed by",
    "removing x from its current cluster c",
    "j",
    "",
    "According to \\(14\\, we know that the value of F is",
    "increased if x is merged into the cluster c which",
    "satisfies the following codition",
    "",
    "",
    "",
    "mi",
    "n",
    "arg",
    "x",
    "c",
    "d",
    "i",
    "F",
    "c",
    "c",
    "i",
    "",
    "",
    "15",
    "So here we can use the algorithm like sequential kmeans to solve our objective function. We start from",
    "the initial random clustering of dataset X. For every",
    "instance x",
    "",
    "X, we remove x from its current cluster c",
    "j",
    "",
    "and merge x into the cluster c which has the minimum",
    "of d",
    "F",
    "c, x\\. We repeat the process until no instance",
    "change its cluster",
    "419",
    "419"
  ],
  "Page#4": [
    "When x is removed from its current c",
    "i",
    "we should",
    "update p\\(c",
    "i",
    "nd p\\(c",
    "q",
    "j",
    "c",
    "i",
    "",
    "l",
    "j",
    "k\\ When x is merged",
    "into c, we should update p\\(c",
    "j",
    "nd p\\(c",
    "q",
    "l",
    "c\\ The update",
    "equation is defined as",
    "",
    "",
    "",
    "",
    "",
    "",
    "c",
    "",
    "1",
    "l",
    "q",
    "j",
    "j",
    "l",
    "j",
    "C",
    "",
    "c",
    "",
    "x",
    "",
    "p",
    "",
    "c",
    "",
    "p",
    "1",
    "",
    "c",
    "",
    "c",
    "",
    "p",
    "q",
    "q",
    "q",
    "c",
    "",
    "16",
    "",
    "",
    "",
    "c",
    "",
    "1",
    "l",
    "l",
    "",
    "x",
    "",
    "p",
    "",
    "c",
    "",
    "p",
    "",
    "",
    "",
    "4. Experiments",
    "",
    "We experiment on four datasets from UCI[1",
    "T",
    "h",
    "e",
    "",
    "attributes in four datasets are numerical. The details of",
    "four datasets are described in Table I",
    "",
    "",
    "TABLE 1 The details of four datasets",
    "",
    "Data",
    "Set",
    "Num. of",
    "Features",
    "Num. of",
    "Classes",
    "Num. of",
    "Instances",
    "Fuzzy",
    "Kmeans",
    "Mean",
    "Error",
    "Rate",
    "Wine 13",
    "3",
    "178",
    "0.314",
    "Glass 9",
    "6",
    "241",
    "0.509",
    "Ionosphere",
    "34 2",
    "351 0.291",
    "Spam",
    "base",
    "57 2",
    "4601 0.357",
    "",
    "In order to produce diverse clusterings, we use",
    "random subspace method[19",
    "",
    "2",
    "0",
    "",
    "",
    "w",
    "h",
    "er",
    "e ea",
    "c",
    "h",
    "b",
    "a",
    "s",
    "e",
    "",
    "clustering is generated on a randomly selected subset",
    "of the original dimensions. Fuzzy k-means is used on",
    "new subspace to produce a “soft” clustering. The",
    "dimension of subspace is",
    "4",
    "",
    "d",
    "The maximum",
    "iterative time in fuzzy k-means is 100",
    "In experiments, we define f-MI when our algorithm",
    "is used to combine “soft” clusterings without",
    "considering the influence of diversity.  We define wMI when our algorithm is used to combine “soft",
    "clusterings with the influence of diversity. We define",
    "h-MI when our algorithm is used to combine “hard",
    "clusterings without considering the influence of",
    "diversity. MCLA, HGPA, CSPA are three ensemble",
    "algorithm proposed by Strehl. They are all based on",
    "hypergraph partitioning. QMI proposed by Topchy is",
    "based on quadratic mutual information criterion and",
    "use k-means to obtain the approximately combined",
    "clustering. We compare w-MI, f-MI, h-MI with",
    "MCLA, HGPA, CSPA and QMI. The CSPA, MCLA",
    "HGPA code is available in [21",
    "",
    "Because MCLA, HGPA, CSPA and QMI are used to",
    "combine “hard” clusterings, they cannot directly",
    "combine “soft” clustering produced by fuzzy k-means",
    "Hard clustering used by MCLA, HGPA, CSPA and",
    "QMI can be produced by assigning every instance to",
    "the cluster in which its conditional probability is at",
    "maximum",
    "Our algorithm is susceptible to the presence of local",
    "minima of the objective functions. To reduce the risk",
    "of convergence to a lower quality solution, the final",
    "clustering was picked from the results of three runs",
    "with random initializations\\ according to the value of",
    "objective function. The highest value of objective",
    "function \\(10\\ served as a criterion for our algorithm",
    "We randomly choose five values [10, 15, 20, 30",
    "",
    "for the size of ensemble",
    "The mean error rate of 10 clustering ensemble",
    "procedures is used to measure the performance of",
    "clustering ensemble. Let C",
    "true",
    "represent the true \\(given",
    "clustering and C the ensemble clustering, “Confusion",
    "be the confusion matrix of two clusterings",
    "Confusion\\(k",
    "true",
    "k\\ = \\(C",
    "ktrue",
    "C",
    "k",
    "i.e. number of points x",
    "that are cluster k",
    "true",
    "in true clustering and cluster k in",
    "the clustering produced. Clustering error rate is defined",
    "as follows",
    "n",
    "k",
    "k",
    "Confusion",
    "n",
    "rate",
    "error",
    "true",
    "true",
    "kk",
    "k",
    "true",
    "",
    "",
    "",
    "",
    "",
    "",
    "1",
    "_",
    "",
    "",
    "",
    "",
    "17",
    "",
    "Where n is the total number of objects. The low",
    "value of error _rate indicates good quality of",
    "clustering",
    "Figure 1,2,3,4 describe shows the mean error rate on",
    "four data sets",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Fig. 1.  The mean error rate on “wine",
    "420",
    "420"
  ],
  "Page#5": [
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Fig.2.   The mean error rate on “glass",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Fig. 3.  The mean error rate on “spambase",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Fig. 4.  The mean error rate on “ionosphere",
    "",
    "From Figure 1,2,3,4, we find that there is no",
    "algorithm which performance is best for all four",
    "datasets with different ensemble size. But on the whole",
    "the performance of w-MI is best among seven",
    "algorithms. The second is f-MI. Especially when the",
    "size of ensemble is small, the mean error rate of f-MI",
    "and w-MI is lower than that of other algorithms",
    "because original “soft” partitions contain much",
    "information than “hard” partitions. Although h-MI and",
    "QMI are both based on mutual information criterion",
    "for “hard” clustering ensemble, h-MI provides clusters",
    "with better quality than QMI",
    "",
    "Table 2. The distribution  of  diversity of",
    "clusterings on “glass",
    "",
    "Times",
    "No",
    "Mean",
    "Standard",
    "Deviation",
    "f-MI w-MI",
    "1 0.7433 0.0395 0.4766 0.4673",
    "2 0.7450 0.0329 0.4346 0.4299",
    "3 0.7827 0.0471 0.5280 0.5280",
    "4 0.667 0.065 0.4766 0.4673",
    "5 0.6789 0.0762 0.5421 0.5374",
    "6 0.7288 0.0550 0.5234 0.5047",
    "7 0.7659 0.0408 0.5467 0.5327",
    "8 0.7450 0.0329 0.4346 0.4299",
    "9 0.7241 0.040 0.4673 0.4555",
    "10 0.7169 0.0516 0.4860 0.4776",
    "The ensemble size is 10. The repeated times of the",
    "ensemble process is 10",
    "",
    "Table 3.The distribution of  diversity of clusterings",
    "on “ionosphere",
    "",
    "Times",
    "No",
    "Mean",
    "Standard",
    "Deviation",
    "f-MI w-MI",
    "1 0.918 0.0243 0.2821 0.2821",
    "2 0.9045 0.012",
    "0.3048 0.3048",
    "3 0.9216 0.0221 0.3077 0.3077",
    "4 0.942 0.0174 0.2906 0.2908",
    "5 0.9086 0.0256 0.2849 0.2849",
    "6 0.9008 0.0151 0.2877 0.2877",
    "7 0.9343 0.0112 0.2934 0.2934",
    "8 0.9059 0.0126 0.2764 0.2764",
    "9 0.9148 0.0163 0.2991 0.2991",
    "10 0.9147 0.0121",
    "0.3105 0.3105",
    "The ensemble size is 10. The repeated times of the",
    "ensemble process is 10",
    "",
    "From figure 1, 2, 3, 4, we also find that the mean",
    "error of w-MI is lower than that of h-MI on most",
    "datasets except “ionosphere”. So on most datasets",
    "except “ionosphere”, considering the influence of",
    "diversity can improve the quality of clustering",
    "ensemble. Why does w-MI not improve the",
    "performance of clustering ensemble on “ionosphere",
    "We list the mean and standard deviation of diversity of",
    "clusterings on “glass” and “ionosphere” in Table 2 and",
    "Table 3. The large standard deviation of diversity",
    "means the distribution of diversity is uneven. Table 2",
    "shows that the standard deviation of diversity",
    "on ”glass” is large and the mean error rate of w-MI is",
    "lower than that of f-MI. Table 3. shows that the",
    "standard deviation of diversity on “ionosphere” is",
    "small but the mean error rate of w-MI is not lower than",
    "421",
    "421"
  ],
  "Page#6": [
    "that of f-MI. So from Table 2 and Table 3, we know",
    "that when the distribution of diversity of clustering is",
    "uneven, the quality of clustering can be improved by",
    "considering the influence of diversity. The distribution",
    "of diversity on “ionosphere” is even, so w-MI can not",
    "improve the performance of clustering ensemble",
    "",
    "5 Conclusion",
    "",
    "In this paper, we extend Strehl’s work and propose a",
    "weighted consensus function to combine “soft",
    "clusterings. In this consensus function, we use mutual",
    "information to measure the sharing information",
    "between two “soft” clusterings and emphasize the",
    "clustering which is much different from the others. We",
    "use the algorithm similar to sequential k-means to",
    "obtain the solution of this consensus function",
    "Experiments on four real data sets indicate that our",
    "consensus function is an effective way to combine",
    "soft” clusterings and when the diversity of clusterings",
    "is uneven, the quality of clustering can be improved by",
    "considering the influence of diversity",
    "",
    "",
    "6. References",
    "",
    "1",
    "",
    "A.L.N. Fred and A.K. Jain, “Data Clustering Using",
    "Evidence Accumulation",
    "In Proc. of the 16th",
    "International Conference on Pattern Recognition, ICPR",
    "2002",
    "Quebec City, pp.276– 280",
    "2",
    "",
    "Fern, X. Z., & Brodley, C. E. “Random projection for",
    "high dimensional data clustering: A cluster ensemble",
    "approach",
    "ICML 2003",
    "Menlo Park, pp.186-193",
    "3",
    "",
    "Monti, S. Tamayo, P, Mesirov, J, & Golub, T",
    "Consensus clustering: A resampling-based method for",
    "class discovery and visualization of gene expression",
    "microarray data",
    "Machine Learning",
    "2003, 52, pp.91",
    "118",
    "4",
    "",
    "A. Topchy, A. Jain, and W. Punch. “A mixture model",
    "for clustering ensembles",
    "In Proc. SIAM Data Mining",
    "",
    "2004, pp.379-390",
    "5",
    "",
    "A. Topchy, A. Jain, and W. Punch. “Combining",
    "multiple weak clusterings",
    "In Proc. Third IEEE",
    "International Conference on Data Mining \\(ICDM'03",
    "",
    "November 2003",
    "6",
    "",
    "Wei Tang, ZhiHua Zhou, “Bagging-Based Selective",
    "Clusterer Ensemble",
    "Journal of software",
    "2005, Vol.16",
    "No.4, pp.496-502",
    "7",
    "",
    "D. Frossyniotis, A. Likas, A. Stafylopatis, “A clustering",
    "method based on boosting",
    "Pattern Recognition Letters",
    "25 \\(2004\\, 641–654",
    "8",
    "",
    "A. Strehl and J. Ghosh. “Cluster ensembles - a",
    "knowledge reuse framework for combining",
    "partitionings",
    "In Proc.Conference on Artificial",
    "Intelligence \\(AAAI 2002",
    "Edmonton, pp.93–98",
    "9",
    "",
    "A. Fred and A.K. Jain. “Data clustering using evidence",
    "accumulation",
    "In Proc. 16th International Conference",
    "on Pattern Recognition, ICPR",
    "Canada, 2002, pp. 276280",
    "",
    "",
    "L.I. Kuncheva, S.T. Hadjitodorov. “Using Diversity in",
    "Cluster Ensem- bles",
    "Proceedings of the IEEE",
    "International Conference on Systems, Man and",
    "Cybernetics",
    "2004,  pp.1214-1219",
    "",
    "",
    "",
    "A. Ben-Hur, A. Elisseeff and I. Guyon, “A stability",
    "based method for discovering structure in clustered",
    "data”, In Pacific Symposium on Biocomputing, 2002",
    "pp.6-17",
    "",
    "",
    "E.B.Fowlkes and C.L.Mallows, “A method for",
    "comparing two hierarchical clusterings",
    "Journal of the",
    "American Statistical Association",
    "1983, 78\\(383",
    "pp.553-569",
    "",
    "",
    "L. Hubert and P. Arabie, “Comparing clusterings",
    "Journal of Classification",
    "1985, 2, pp.193-218",
    "",
    "",
    "W.M. Rand, “Objective criteria for the evaluation of",
    "clustering methods",
    "Journal of the American Statistical",
    "Association",
    "1971\\(66\\, pp.846-850",
    "",
    "",
    "S. Dongen, “Performance criteria for graph clustering",
    "and Markov cluster experiments",
    "Technical Report",
    "",
    "Amsterdam, 2000",
    "",
    "",
    "M. Meila. “Comparing clusterings by the variation of",
    "information",
    "In Conference on Learning",
    "Theory\\(COLT",
    "2003, pp.173-187",
    "",
    "",
    "Marina Meila. “Comparing clusterings: an axiomatic",
    "view",
    "In International Conference of Machine",
    "Learning \\(ICML",
    "2005: 577-584",
    "",
    "",
    "Blake C, Keogh E, Merz CJ. UCI Repository of",
    "machine learning databases. Irvine: Department of",
    "Information and Computer Science, University of",
    "California, [Online",
    "A",
    "v",
    "a",
    "i",
    "l",
    "a",
    "b",
    "l",
    "e",
    "",
    "h",
    "t",
    "t",
    "p",
    "",
    "",
    "",
    "www",
    "i",
    "c",
    "s",
    "",
    "u",
    "c",
    "i",
    "",
    "e",
    "d",
    "u",
    "",
    "mlearn/MLRepository.html",
    "",
    "",
    "D. Greene, A. Tsymbal, N. Bolshakova, P",
    "Cunningham, “Ensemble clustering in medical",
    "diagnostics",
    "In",
    "",
    "Proc. 17th IEEE Symp. on ComputerBased Medical Systems CBMS 2004, Bethesda, MD",
    "National Library of Medicine/National Institutes of",
    "Health",
    "IEEE CS Press, 2004, pp.576–581",
    "",
    "",
    "T. K. Ho. “The random subspace method for",
    "constructing decision forests",
    "IEEE Transactions on",
    "Pattern Analysis and Machine Intelligence",
    "1998, 20\\(8",
    "pp.832–844",
    "",
    "",
    "CSPA, MCLA, HGPA code[Online",
    "A",
    "v",
    "a",
    "ila",
    "b",
    "l",
    "e",
    "",
    "",
    "http://strehl.com/75-280, 2001",
    "422",
    "422"
  ],
  "Page#7": [
    "",
    "r\\022\\023\\024\\025\\026\\006\\027\\030",
    "",
    "025\\f\\003\\033\\017!\\024\\030\\f\\003\\021\\025\\023&\\021\\003\\021\\023\\031\\f\\003\\021\\016\\031\\027\\020\\f\\003\\027\\016\\r\\r\\f\\030\\034\\021\\003\\036\\016# \\037\\003\\033\\030\\023\\031\\003\\r\\025\\f\\003",
    "r\\030\\016\\017\\034\\017\\034!\\003",
    "021\\f\\r\\003",
    "023\\033\\003",
    "r\\025\\f\\003",
    "027\\030\\023\\022\\020\\f\\031\\003",
    "017\\034\\021\\r\\016\\034 \\f\\003",
    "005\\005\\006\\003",
    "017\\r\\025\\003",
    "020\\017\\034\\\\f\\032\\003",
    "016\\030\\030\\023&\\021\\003",
    "017\\034\\032\\017 \\016\\r\\017\\034!\\003",
    "r\\025\\f\\003",
    "030\\f\\020\\f\\(\\016\\034\\r\\003",
    "021\\024\\022\\021\\r\\030\\024 \\r\\024\\030\\f\\021\\006\\003",
    "016\\034\\032\\003",
    "r\\025\\f\\003",
    "f\\034\\r\\030\\023\\017\\032\\003",
    "036\\032\\037\\003",
    "023\\033\\003",
    "r\\025\\f\\003",
    "021*\\031\\022\\023\\020\\003",
    "021\\f\\020\\f \\r\\f\\032\\003",
    "022*\\003",
    "r\\025\\f\\003",
    "f\\034\\f\\r\\017 \\003",
    "016\\020!\\023\\030\\017\\r\\025\\031\\003",
    "r\\023\\003",
    "032\\017\\021 \\030\\017\\031\\017\\034\\016\\r\\f\\003",
    "r\\025\\f\\003",
    "020\\016\\021\\021\\f\\021\\035\\003",
    "025\\f\\003",
    "021*\\031\\022\\023\\020\\003",
    "023\\030\\030\\f\\021\\027\\023\\034\\032\\021\\003",
    "r\\023\\003",
    "r\\025\\f\\003",
    "030\\f\\020\\f\\(\\016\\034\\r\\003",
    "021\\024\\022\\021\\r\\030\\024 \\r\\024\\030\\f\\003",
    "033\\023\\024\\034\\032\\003",
    "017\\034\\003",
    "020\\016\\021\\021\\003",
    "002\\003",
    "016\\034\\032\\003",
    "025\\016\\021\\003",
    "016\\034\\003",
    "016\\021\\021\\023 \\017\\016\\r\\f\\032\\003",
    "031\\f\\r\\030\\017 \\003",
    "027\\016\\030\\016\\031\\f\\r\\f\\030\\003",
    "021\\f\\r\\003",
    "025\\017 \\025\\003",
    "017\\(\\f\\021\\003",
    "016\\003",
    "034\\023\\034#\\034\\024\\020\\020\\003",
    "f\\017!\\025\\r\\003",
    "r\\023\\003",
    "r\\025\\f\\003",
    "021\\f \\r\\017\\023\\034\\021\\003",
    "031",
    "005",
    "003",
    "016\\034\\032\\003",
    "026",
    "t",
    "006\\003",
    "016\\021\\003",
    "017\\r\\003",
    "021\\025\\023\\024\\020\\032\\003",
    "022\\f\\006\\003",
    "021\\017\\034 \\f\\003",
    "r\\025\\f\\003",
    "023\\020\\023\\030\\003",
    "016\\034\\032\\003",
    "016\\022\\021\\023\\020\\024\\r\\f\\003",
    "027\\023\\021\\017\\r\\017\\023\\034\\003",
    "023\\033\\003",
    "r\\025\\f\\003",
    "034\\023\\032\\f\\021\\003",
    "023\\031\\027\\023\\021\\017\\034!\\003\\r\\025\\f\\003\\021\\024\\022\\021\\r\\030\\024 \\r\\024\\030\\f\\003\\032\\023\\f\\021\\034+\\r\\003 \\025\\016\\030\\016 \\r\\f\\030\\017,\\f\\003\\r\\025\\f\\003\\021*\\031\\022\\023\\020\\003\\016\\034\\032\\003",
    "016\\034\\003\\016\\021\\021\\024\\031\\f\\003\\032\\017\\033\\033\\f\\030\\f\\034\\r\\003\\036\\030\\016\\034\\032\\023\\031\\037\\003\\(\\016\\020\\024\\f\\021\\003\\r\\025\\030\\023\\024!\\025\\023\\024\\r\\003\\r\\025\\f\\003&\\025\\023\\020\\f\\003\\021\\f\\r\\003\\023\\033\\003",
    "020\\016\\021\\021\\003",
    "f\\026\\016\\031\\027\\020\\f\\021\\035\\003",
    "f\\003",
    "016\\034\\003",
    "023\\022\\021\\f\\030\\(\\f\\003",
    "025\\023&\\003",
    "016\\020\\020\\003",
    "r\\025\\f\\003",
    "034\\023\\032\\f\\003",
    "016\\034\\032\\003",
    "f\\032!\\f\\003",
    "033\\f\\016\\r\\024\\030\\f\\021\\003\\025\\016\\(\\f\\003\\022\\f\\f\\034\\003\\016\\(\\f\\030\\016!\\f\\032\\003\\r\\023\\003\\033\\023\\030\\031\\003\\r\\025\\f\\003\\027\\030\\023\\r\\023\\r*\\027\\f\\035",
    "",
    "",
    "These",
    "results",
    "confirm",
    "that",
    "the",
    "useful",
    "information",
    "contained",
    "in",
    "tr",
    "S",
    "210",
    "and",
    "ts",
    "S",
    "210",
    "becomes harder",
    "to extract for",
    "the",
    "heuristic",
    "search",
    "engine",
    "as",
    "noise",
    "increases",
    "Thus",
    "more",
    "complex",
    "and",
    "accurate",
    "search",
    "mechanisms",
    "could",
    "improve",
    "the",
    "overall",
    "performance",
    "All",
    "the",
    "tests",
    "have",
    "been",
    "carried",
    "out",
    "on",
    "a",
    "PC",
    "equipped",
    "with",
    "AMD",
    "Athlon\\231",
    "64",
    "2800",
    "1.81",
    "GHz",
    "CPU",
    "1",
    "GB",
    "of",
    "RAM",
    "software",
    "has",
    "been",
    "developed",
    "with",
    "the",
    "Borland",
    "C",
    "Builder",
    "6\\231",
    "compiler",
    "under",
    "the",
    "Windows",
    "XP\\231",
    "operative",
    "system",
    "The",
    "time",
    "required",
    "for",
    "the",
    "symbols",
    "discovery",
    "procedure",
    "ranges",
    "from",
    "16\\222\\222",
    "to",
    "35\\222",
    "1\\222\\222",
    "depending on the problem instance, with an average of",
    "10\\222 33\\222\\222. The time required for the symbol recognition",
    "on a single pattern ranges from 0.001\\222\\222 to 5.831\\222\\222 with",
    "an",
    "average",
    "of",
    "",
    "0.708\\222\\222",
    "The",
    "time",
    "required",
    "for",
    "a",
    "single",
    "run",
    "of",
    "the genetic search",
    "algorithm",
    "including",
    "the cost",
    "evaluation",
    "of",
    "the",
    "candidate",
    "models",
    "ranges",
    "from",
    "0.292\\222\\222 to 31.833\\222\\222, with an average of  7.587\\222\\222",
    "As",
    "it",
    "can",
    "be",
    "observed",
    "good",
    "performances",
    "are",
    "obtained",
    "even",
    "in",
    "the",
    "instances",
    "derived",
    "from",
    "the",
    "problem definition D6, which is based on substructures",
    "of",
    "size",
    "equal",
    "to",
    "7",
    "In",
    "fact",
    "even",
    "if",
    "3",
    "max",
    "",
    "d",
    "",
    "the",
    "problem",
    "can",
    "be",
    "solved",
    "because",
    "class-discriminating",
    "smaller subgraphs of the substructures  can be isolated",
    "However",
    "as",
    "concerns",
    "the",
    "semantic",
    "information",
    "extraction",
    "the",
    "solution",
    "can",
    "be",
    "considered",
    "incomplete",
    "because",
    "the",
    "synthesized",
    "prototypes",
    "represent",
    "just",
    "subgraphs of the real significant structures",
    "",
    "5. Conclusions and future work",
    "",
    "In",
    "this",
    "paper",
    "we",
    "propose",
    "a",
    "symbolic",
    "classification",
    "system based on an information granulation procedure",
    "able",
    "to",
    "deal",
    "with",
    "graphs",
    "with",
    "node",
    "and",
    "edge",
    "labels",
    "belonging",
    "to",
    "continuous",
    "vector",
    "spaces",
    "The",
    "overall",
    "classification",
    "technique",
    "is",
    "general",
    "enough",
    "to",
    "be",
    "employed",
    "on",
    "a",
    "wide",
    "range",
    "of",
    "problems",
    "coming",
    "from",
    "real",
    "world",
    "applications",
    "It",
    "can",
    "be",
    "thought",
    "as",
    "an",
    "essential",
    "step",
    "to",
    "define",
    "a",
    "unified",
    "granular",
    "modeling",
    "approach",
    "The",
    "system",
    "is",
    "able",
    "to",
    "solve",
    "a",
    "large",
    "set",
    "of",
    "different",
    "classification",
    "problems",
    "automatically",
    "i.e",
    "without",
    "any",
    "need",
    "to",
    "set",
    "up",
    "system",
    "parameters",
    "on",
    "the",
    "basis of the problem at hand. In general, our approach",
    "showed a good aptitude to robustness, since even noisy",
    "problems",
    "have",
    "been",
    "solved",
    "without",
    "any",
    "need",
    "to",
    "adapt",
    "system parameters to the noise level. The extraction of",
    "relevant substructure prototypes, each characterized by",
    "its",
    "own",
    "local",
    "metric",
    "parameters",
    "appeared",
    "to",
    "be",
    "a",
    "powerful",
    "framework",
    "for",
    "the",
    "synthesis",
    "of",
    "alternative",
    "representations",
    "of",
    "the",
    "original",
    "graphs",
    "In",
    "fact",
    "the",
    "symbolic",
    "histogram",
    "is",
    "a",
    "high",
    "semantic",
    "level",
    "representation",
    "that",
    "can",
    "be",
    "used",
    "to",
    "feed",
    "directly",
    "classification systems working on metric spaces",
    "",
    "6. References",
    "",
    "1",
    "",
    "A",
    "Rizzi",
    "G",
    "Del",
    "Vescovo",
    "223Automatic",
    "Image",
    "Classification",
    "by",
    "a",
    "Granular",
    "Computing",
    "Approach\\224",
    "in",
    "Proc",
    "IEEE",
    "International",
    "Workshop",
    "on",
    "Machine",
    "Learning",
    "for",
    "Signal",
    "Processing",
    "",
    "Maynooth",
    "Ireland",
    "2006, pp. 33 - 38",
    "2",
    "",
    "M",
    "Deshpande",
    "M",
    "Kuramochi",
    "N",
    "Wale",
    "G",
    "Karypis",
    "223Frequent substructure-based approaches for classifying",
    "chemical",
    "compounds\\224",
    "IEEE",
    "Trans",
    "Knowledge",
    "and",
    "Data",
    "Engineering",
    "",
    "vol",
    "17",
    "no",
    "8",
    "pp",
    "1036\\2261050",
    "August. 2005",
    "3",
    "",
    "L",
    "Holder",
    "D",
    "Cook",
    "J",
    "Gonzales",
    "I",
    "Jonyer",
    "Structural",
    "Pattern Recognition",
    "in Graphs\" in",
    "D. Chen",
    "X. Chengs",
    "Eds",
    "Pattern Recognition and String matching",
    "Series",
    "Combinatorial",
    "Optimization",
    "Vol",
    "13",
    "ISBN",
    "978-14020-0953-2",
    "Kluwer",
    "Academic",
    "Publishers",
    "pp",
    "255279, 2003",
    "4",
    "",
    "X",
    "Jiang",
    "A",
    "M\\374nger",
    "H",
    "Bunke",
    "223On",
    "Median",
    "Graphs",
    "Properties",
    "Algorithms",
    "and",
    "Applications\\224",
    "in",
    "IEEE",
    "Trans. Pattern Analysis and Machine Intelligence",
    "Vol",
    "23, No. 10, October 2001, pp. 1144-1151",
    "",
    "016\\037",
    "003",
    "017\\030\\021\\r\\003\\021\\016\\031\\027\\020\\f\\003\\023\\033\\003 \\020\\016\\021\\021\\003\\002",
    "003",
    "022\\037",
    "003",
    "f \\023\\034\\032\\003\\021\\016\\031\\027\\020\\f\\003\\023\\033\\003 \\020\\016\\021\\021\\003\\002",
    "003",
    "037",
    "003",
    "016\\031\\027\\020\\f\\003\\023\\033\\003 \\020\\016\\021\\021\\003\\005",
    "032\\037\\003",
    "034\\r\\025\\f\\021\\017,\\f\\032\\003\\027\\030\\023\\r\\023\\r*\\027\\f\\003\\036 \\020\\016\\021\\021\\003\\002\\037",
    "003",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "416",
    "416"
  ]
}