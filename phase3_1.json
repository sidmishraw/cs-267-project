{
  "Page#1": {
    "Content#0": {
      "##": [
        "MongoDB Sharding",
        "",
        "1",
        "CS185C: Introduction to NoSQL Databases",
        "Suneuy Kim"
      ]
    }
  },
  "Page#2": {
    "Content#0": {
      "##": [
        "Vertical Scaling vs. Horizontal Scaling",
        "",
        "Vertical Scaling ",
        "",
        "Increasing the capacity of a single server by adding more ",
        "RAM, using a more powerful CPU, or increasing the ",
        "amount of storage space. ",
        "",
        "There is a practical maximum for vertical scaling",
        "",
        "Horizontal Scaling",
        "",
        "Divides the system dataset and load over multiple servers, ",
        "adding additional servers to increase capacity",
        "",
        "Increases complexity in infrastructure and maintenance. ",
        "",
        "MongoDB supports horizontal scaling through sharding. ",
        "",
        "2"
      ]
    }
  },
  "Page#3": {
    "Content#0": {
      "##": [
        "When to shard",
        "",
        "To increase available RAM",
        "",
        "To increase available disk space",
        "",
        "To reduce load on a server",
        "",
        "To read or write data with greater throughput than a single ",
        "mongod",
        "can handle ",
        "",
        "3"
      ]
    }
  },
  "Page#4": {
    "Content#0": {
      "##": [
        "Sharding \\(= horizontal partitioning\\ ",
        "",
        "The process of splitting up a large collection, i.e. containing a ",
        "large number of documents, across multiple independent  ",
        "servers to improve performance. ",
        "",
        "MongoDB implements sharding at the collection level, not ",
        "the database level. ",
        "",
        "MongoDB adopts auto",
        "-",
        "sharding ",
        "",
        "A cloud of shards can be treated as a single logical ",
        "database. ",
        "",
        "An application is not aware of that the data are ",
        "distributed across multiple servers. ",
        "",
        "4"
      ]
    }
  },
  "Page#5": {
    "Content#0": {
      "##": [
        "Requirements of Sharding System",
        "1.",
        "The ability to distribute data evenly across all shards ",
        "-",
        "balancer",
        "2.",
        "The ability to store shard data in a fault",
        "-",
        "tolerant fashion ",
        "-",
        "The ability to use replica sets as a storage mechanism for shards",
        "3. The ability to add or remove shards while the system is running ",
        "",
        "config",
        "server",
        "",
        "5"
      ]
    }
  },
  "Page#6": {
    "Content#0": {
      "##": [
        "Sharded",
        "Cluster Components",
        "",
        "6",
        "",
        "Shard",
        "",
        "Each shard contains a subset of the ",
        "sharded",
        "data. ",
        "",
        "Each shard can be deployed as a replica set.",
        "",
        "C",
        "onfig",
        "servers",
        "",
        "A ",
        "config",
        "server, which is a ",
        "mongod",
        "server, acts as a directory that allows ",
        "the location of each chunk to be determined.  ",
        "",
        "For this, a ",
        "c",
        "onfig",
        "server ",
        "stores metadata and configuration settings for ",
        "the cluster. ",
        "",
        "As of MongoDB 3.2, ",
        "config",
        "servers can be deployed as a replica set.      ",
        "\\(",
        "3 ",
        "config",
        "servers are ",
        "recommended.\\",
        "",
        "m",
        "ongos ",
        "",
        "next page"
      ]
    }
  },
  "Page#7": {
    "Content#0": {
      "##": [
        "mongos ",
        "",
        "The mongos acts as a query router, providing an interface between ",
        "client applications and the ",
        "sharded",
        "cluster",
        ".",
        "",
        "A ",
        "mongos",
        "routing process manages ",
        "1\\",
        "splitting of data",
        "2\\",
        "routing of requests to the required shard server",
        "3\\",
        "merging the data from multiple shards to answer a query",
        "",
        "Applications connect to the mongos and issue requests to it. ",
        "",
        "The shard key maps data into chunks. ",
        "",
        "Chunks are ",
        "logical",
        "contiguous ranges of document keys. ",
        "",
        "Each chunk identifies a number of documents with a particular ",
        "continuous range of sharding key values. ",
        "",
        "7"
      ]
    }
  },
  "Page#8": {
    "Content#0": {
      "##": [
        "Simple sharding setup without redundancy",
        "The instruction for this set up is ",
        "presented in a separate document",
        "",
        "",
        "",
        "8"
      ]
    }
  },
  "Page#9": {
    "Content#0": {
      "##": [
        "clients",
        "Replica Set ",
        "",
        "mongod",
        "Config",
        "Servers",
        "mongos",
        "Shard Controllers",
        "Replica Set",
        "-",
        "Shard 01",
        "Replica Set",
        "-",
        "Shard 02",
        "Replica Set",
        "-",
        "Shard 03",
        "A redundant sharding ",
        "configuration",
        "m",
        "ongod",
        "-",
        "Config",
        "Server",
        "mongos",
        "-",
        "Shard ",
        "Controller",
        "m",
        "ongod",
        "-",
        "Config",
        "Server",
        "mongos",
        "-",
        "Shard ",
        "Controller",
        "m",
        "ongod",
        "-",
        "Config",
        "Server",
        "mongos",
        "-",
        "Shard ",
        "Controller",
        "clients",
        "When these services are employed ",
        "on three physical servers. ",
        "",
        "",
        "9",
        "",
        "Common setup: One mongos per ",
        "application server running on the same ",
        "machine as the application server. ",
        "m",
        "ongod",
        "Shard Store",
        "m",
        "ongod",
        "Shard Store",
        "m",
        "ongod",
        "Shard Store",
        "m",
        "ongod",
        "Shard Store",
        "m",
        "ongod",
        "Shard Store",
        "m",
        "ongod",
        "Shard Store",
        "m",
        "ongod",
        "Shard Store",
        "m",
        "ongod",
        "Shard Store",
        "m",
        "ongod",
        "Shard Store"
      ]
    }
  },
  "Page#10": {
    "Content#0": {
      "##": [
        "Sharding Data",
        "",
        "You must explicitly tell both the database and collection that you want ",
        "them to be distributed. ",
        "",
        "Sharding database is always prerequisite to sharding one of its collections",
        "",
        ">",
        "db.enableSharding",
        "",
        ">",
        "sh.shardCollection",
        "",
        "mustic.artists",
        "",
        "",
        "The ",
        "shardCollection",
        "command splits the collection into chunks ",
        "",
        "For an existing collection, there must be an index on the shard key. ",
        "Otherwise, an error occurs. ",
        "",
        "For a non",
        "-",
        "existing collection, an index on the shard key is automatically ",
        "created. ",
        "",
        "10"
      ]
    }
  },
  "Page#11": {
    "Content#0": {
      "##": [
        "Chunks",
        "",
        "The unit MongoDB uses to move data around",
        "",
        "A group of documents in a given range of the shard key",
        "",
        "A chunk always lives on a single shard. ",
        "",
        "Once a chunk grows to a certain size, due to insertions, ",
        "MongoDB automatically splits it into two smaller chunks. ",
        "",
        "Chunks are exclusive without any overlapping ranges. ",
        "",
        "A document belongs one and only one chunk ",
        "",
        "an array ",
        "field cannot be used as a shard key. ",
        "",
        "A chunk is not necessarily grouped together on disk. ",
        "",
        "11"
      ]
    }
  },
  "Page#12": {
    "Content#0": {
      "##": [
        "Chunk Ranges",
        "",
        "A newly ",
        "sharded",
        "collection starts off with a single chunk.",
        "",
        "As a chunk grows, it is automatically split into two chunks. ",
        "Example: A chunk with a range 3<= age < 17 splits into two ranges: 3<=age < 12 on ",
        "one chunk and 12<=age < 17 on the other. 12 is the ",
        "split point",
        ". ",
        "",
        "Assignment: compound sharding key",
        "1\\",
        "Setup a ",
        "sharded",
        "collection with 2 shards using a compound shard key ",
        "on ",
        "",
        "\\Use chunk size 1. ",
        "2\\",
        "Populate this collection documents consisting of username and age. ",
        "3\\",
        "Show the chunk information of this collection",
        "4\\",
        "Elaborate what type of query can benefits from this ",
        "sharded",
        "collection. ",
        "E.g. To find on which chunk someone with a given user name vs.",
        "To find one which chunk someone with a given age",
        "",
        "12"
      ]
    }
  },
  "Page#13": {
    "Content#0": {
      "##": [
        "Chunk Ranges",
        "",
        "Chunk information is stored in the ",
        "config.chunks",
        "collection.",
        "vagrant@vagrant",
        "-",
        "ubuntu",
        "-",
        "trusty",
        "-",
        "64",
        ":~$ ",
        "sudo",
        "mongo ",
        "127.0.0.1:27021",
        "mongos",
        "> use ",
        "config",
        "mongos> ",
        "db.",
        "chunks",
        ".find",
        "\\(\\",
        "{ \"_id\" : \"",
        "testdb.testcollection",
        "-",
        "testkey_MinKey",
        "\", \"",
        "lastmod",
        "\" : Timestamp\\(10, 0\\",
        "\"",
        "lastmodEpoch",
        "\" : ",
        "ObjectId",
        "\\(\"5858a1e8b4ccc8eb41074bca\"\\\"ns\" : ",
        "\"",
        "testdb.testcollection",
        "\", \"",
        "min",
        "\" : { \"",
        "testkey",
        "\" : { ",
        "\"$",
        "minKey",
        "\" : 1 } }, \"",
        "max",
        "\" : { \"",
        "testkey",
        "\" : ",
        "\"",
        "key0",
        "\" }, \"shard\" : \"shard0000\" }",
        "{ \"_id\" : \"",
        "testdb.testcollection",
        "-",
        "testkey",
        "_",
        "\\\\",
        "\"key0",
        "\\\\",
        "\"\", \"",
        "lastmod",
        "\" : Timestamp\\(8, 1\\",
        "\"",
        "lastmodEpoch",
        "\" : ",
        "ObjectId",
        "\\(\"5858a1e8b4ccc8eb41074bca\"\\\"ns\" : ",
        "\"",
        "testdb.testcollection",
        "\", \"",
        "min",
        "\" : { \"",
        "testkey",
        "\" : \"",
        "key0",
        "\" }, \"",
        "max",
        "\" : { \"",
        "testkey",
        "\" : ",
        "\"",
        "key19970",
        "\" }, \"shard\" : \"shard0000\" ",
        "}",
        "",
        "",
        "on the course ",
        "web site. ",
        "",
        "13"
      ]
    }
  },
  "Page#14": {
    "Content#0": {
      "##": [
        "Splitting Chunks",
        "",
        "14",
        "",
        "m",
        "onogs",
        "tracks how much ",
        "data it writes  per chunk ",
        "and ",
        "",
        "If the split threshold is ",
        "reached, mongos will ",
        "send a request for split ",
        "points to the shard. ",
        "",
        "The shard calculate split ",
        "points for the chunk and ",
        "sends them to the ",
        "mongos"
      ]
    }
  },
  "Page#15": {
    "Content#0": {
      "##": [
        "Splitting chunks",
        "",
        "Chunk splits are just a metadata change \\(no data move\\in the ",
        "config",
        "server. ",
        "",
        "After new chunk documents are created on the ",
        "config",
        "servers and the ",
        "",
        "for the original chunk and creates new trackers for the new chunk",
        "",
        "",
        "value changes ",
        "",
        "documents with the same shard key must live in the ",
        "same chunk",
        "",
        "15",
        "c",
        "annot split here. ",
        "",
        "Having a variety values for ",
        "a shard key is important. ",
        ""
      ]
    }
  },
  "Page#16": {
    "Content#0": {
      "##": [
        "Split storm",
        "",
        "16",
        "",
        "All ",
        "config",
        "servers must be up for splits to happen. ",
        "",
        "If a ",
        "config",
        "server is down when a mongos tries to do a split, ",
        "the mongos cannot update the metadata and the split fails. ",
        "",
        "Ensure your ",
        "config",
        "servers are up and healthy not to have a split storm. "
      ]
    }
  },
  "Page#17": {
    "Content#0": {
      "##": [
        "Case where chunks grow without bound",
        "",
        "17",
        "1.",
        "Leave mongos processes up.",
        "2. Make a chunk size smaller in a way ",
        "that mongos prompts splits to happen",
        "at a lower threshold.  "
      ]
    }
  },
  "Page#18": {
    "Content#0": {
      "##": [
        "Config",
        "servers ",
        "",
        "Config",
        "servers store the metadata for a ",
        "sharded",
        "cluster. ",
        "",
        "The ",
        "metadata includes the list of chunks on every shard and the ranges that ",
        "define the chunks",
        ".",
        "",
        "Config",
        "servers must be set up first and the metadata they hold is extremely ",
        "important. ",
        "",
        "Three ",
        "config",
        "servers are recommended. ",
        "",
        "The mongos instances cache this data and use it to route read and write ",
        "operations to the correct shards. ",
        "",
        "A mongos ",
        "reads data from the ",
        "config",
        "server in the following cases:",
        "",
        "A ",
        "new mongos starts for the first time, or an existing mongos ",
        "restarts.",
        "",
        "After ",
        "change in the cluster metadata, such as after a chunk migration",
        ".",
        "",
        "A mongos ",
        "only writes data to the ",
        "config",
        "servers when the metadata changes, ",
        "such as",
        "",
        "after a ",
        "chunk migration or after ",
        "a ",
        "chunk split",
        "",
        "18"
      ]
    }
  },
  "Page#19": {
    "Content#0": {
      "##": [
        "The balancer",
        "",
        "As an element of mongos process, it checks its table of chunks for ",
        "each collection to see if any shards have hit the balancing threshold.  ",
        "",
        "If an imbalance is detected, the balancer redistributes chunks within a ",
        "cluster \\(",
        "migration",
        "\\to ensure ",
        "even distribution of data ",
        "among shards. ",
        "",
        "Migration in progress is not perceived by an application.  All reads ",
        "and writes are routed to the old chunk until the migration is ",
        "complete. ",
        "",
        "Once migration is done, all mongos attempting to access the data in ",
        "the old location will get an error. If then, mongos looks up the new ",
        "location of the data from ",
        "config",
        "servers, updates its chunk table, and ",
        "tries the request again. ",
        "",
        "19"
      ]
    }
  },
  "Page#20": {
    "Content#0": {
      "##": [
        "The balancer",
        "mongos> ",
        "sh.stopBalancer",
        "\\(\\",
        "Waiting for active hosts...",
        "Waiting for the balancer lock...",
        "Waiting again for active hosts after balancer is off...",
        "WriteResult",
        "\\({ \"",
        "nMatched",
        "\" : 1, \"",
        "nUpserted",
        "\" : 0, \"",
        "nModified",
        "\" : 1 }\\",
        "mongos> ",
        "db.settings.find",
        "\\({_",
        "id:\"balancer",
        "\"}\\",
        "{ \"_id\" : \"balancer\", \"stopped\" : true }",
        "mongos> ",
        "sh.startBalancer",
        "\\(\\",
        "WriteResult",
        "\\({ \"",
        "nMatched",
        "\" : 1, \"",
        "nUpserted",
        "\" : 0, \"",
        "nModified",
        "\" : 1 }\\",
        "",
        "20"
      ]
    }
  },
  "Page#21": {
    "Content#0": {
      "##": [
        "Choosing a shard key ",
        "",
        "Shard key: a field or two to use to split up the data",
        "",
        "Questions to ask to choose a shard key",
        "",
        "How many shards are you planning to grow?",
        "",
        "Are you sharding to reduce read/write latency? ",
        "",
        "Are you sharding to increase read/write throughput?",
        "",
        "Are you sharding to increase system resources?",
        "",
        "Basic distributions: ascending key, random and location",
        "-",
        "based ",
        "",
        "classification based on the nature of key values",
        "",
        "21"
      ]
    }
  },
  "Page#22": {
    "Content#0": {
      "##": [
        "Ascending Shard Keys",
        "",
        "A key that steadily increases over time. e.g. ",
        "ObjectId",
        "",
        "Max chunk: the chunk containing ",
        "$",
        "maxKey",
        "",
        "With this pattern, all the newly added documents will be in ",
        "the max chunk. ",
        "",
        "All of your writes will be routed to one shard containing ",
        "the max chunk. ",
        "",
        "The max chunk continues growing and being split into ",
        "multiple chunks. ",
        "",
        "All the chunks are being created by one shard ",
        "",
        "difficult for ",
        "MongoDB to keep chunks evenly balanced. ",
        "",
        "22"
      ]
    }
  },
  "Page#23": {
    "Content#0": {
      "##": [
        "",
        "23"
      ]
    }
  },
  "Page#24": {
    "Content#0": {
      "##": [
        "Ascending Shard Keys",
        "",
        "24"
      ]
    }
  },
  "Page#25": {
    "Content#0": {
      "##": [
        "Randomly Distributed Shard Keys",
        "",
        "Keys that has no identifiable pattern in your dataset. ",
        "Examples: Usernames, email addresses, UUID, MD5 ",
        "hashes, etc.",
        "",
        "Pro: Inserts should hit every chunk fairly evenly and ",
        "shards grow roughly the same rate ",
        "",
        "the number of ",
        "migrates will be limited. ",
        "",
        "Con: MongoDB is not efficient at randomly accessing ",
        "data beyond the size of RAM. ",
        "",
        "25"
      ]
    }
  },
  "Page#26": {
    "Content#0": {
      "##": [
        "",
        "26",
        "Randomly ",
        "Distributed Shard ",
        "Keys"
      ]
    }
  },
  "Page#27": {
    "Content#0": {
      "##": [
        "Location",
        "-",
        "Based Shard Keys",
        "",
        "A location",
        "-",
        "based key is a key that makes documents with some ",
        "similarity fall into a range based on this field. ",
        "",
        "A location does not necessarily mean a physical location field ",
        "",
        "",
        "",
        "27"
      ]
    }
  },
  "Page#28": {
    "Content#0": {
      "##": [
        "Shard Key Strategies",
        "",
        "MongoDB supports two sharding strategies for ",
        "distributing data ",
        "across ",
        "sharded",
        "clusters.",
        "",
        "Hashed Sharding ",
        "",
        "Ranged Sharding ",
        "",
        "28"
      ]
    }
  },
  "Page#29": {
    "Content#0": {
      "##": [
        "Hashed Sharding",
        "",
        "Hashed sharding uses a  hashed index of a single field as the ",
        "shard key to ",
        "partition data across your ",
        "sharded",
        "cluster",
        ".",
        "",
        "Each ",
        "chunk is then assigned a range based on the hashed ",
        "shard key values.",
        "",
        "Hashed sharding ",
        "facilitates more even data distribution, ",
        "especially in data sets ",
        "with a ascending shard key. ",
        "",
        "Limitations",
        "",
        "Random data \\(and index\\updates can be I/O intensive. ",
        "",
        "Range queries are ",
        "costy",
        ". The ",
        "mongos is more likely to ",
        "perform Broadcast Operations to fulfill a given ranged ",
        "query. ",
        "",
        "29"
      ]
    }
  },
  "Page#30": {
    "Content#0": {
      "##": [
        "Hashed Sharding",
        "",
        "30"
      ]
    }
  },
  "Page#31": {
    "Content#0": {
      "##": [
        "mongos> ",
        "db.users.ensureIndex",
        "\\({\"",
        "username",
        "\":\"hashed",
        "\"}\\",
        "{",
        "\"raw\" : {",
        "\"127.0.0.1:27024\" : {",
        "\"",
        "createdCollectionAutomatically",
        "\" : true,",
        "\"",
        "numIndexesBefore",
        "\" : 1,",
        "\"",
        "numIndexesAfter",
        "\" : 2,",
        "\"ok\" : 1",
        "}",
        "},",
        "\"ok\" : 1",
        "}",
        "",
        "31",
        "To create a hashed index on the shard key"
      ]
    }
  },
  "Page#32": {
    "Content#0": {
      "##": [
        "Shard an empty collection ",
        "testdb.users",
        "mongos> show databases",
        "config",
        "0.001GB",
        "testdb",
        "0.005GB",
        "mongos> use ",
        "testdb",
        "switched to ",
        "db",
        "testdb",
        "mongos> show collections",
        "testcollection",
        "testtagsharding",
        "users",
        "mongos>",
        "sh.shardCollection",
        "\\(\"",
        "testdb.users",
        "\",{\"",
        "username\":\"hashed",
        "\"}\\",
        "{ \"",
        "collectionsharded",
        "\" : \"",
        "testdb.users",
        "\", \"ok\" : 1 ",
        "}",
        "",
        "32"
      ]
    }
  },
  "Page#33": {
    "Content#0": {
      "##": [
        "When ",
        "a hashed shard key is created on ",
        "an empty ",
        "collection, ",
        "shardCollection",
        "creates ",
        "two",
        "chunks per shard. ",
        "m",
        "ongos> ",
        "sh.status",
        "\\(\\",
        "testdb.users",
        "shard key: { \"username\" : \"hashed\" ",
        "} . . .",
        "chunks:  shard0000       ",
        "2",
        "shard0001       ",
        "2",
        "{ ",
        "\"username\" : { \"$",
        "minKey",
        "\" : 1 } } ",
        "--",
        ">> { \"username\" : ",
        "NumberLong",
        "\\(\"",
        "-",
        "4611686018427387902\"\\ } on : ",
        "shard0000",
        "Timestamp\\(2, 2\\",
        "{ \"username\" : ",
        "NumberLong",
        "\\(\"",
        "-",
        "4611686018427387902\"\\ } ",
        "--",
        ">> { ",
        "\"username\" : ",
        "NumberLong",
        "\\(0\\} on : ",
        "shard0000",
        "Timestamp\\(2, 3\\",
        "{ \"username\" : ",
        "NumberLong",
        "\\(0\\} ",
        "--",
        ">> { \"username\" : ",
        "NumberLong",
        "\\(\"4611686018427387902\"\\on : ",
        "shard0001",
        "Timestamp\\(2, 4\\",
        "{ \"username\" : ",
        "NumberLong",
        "\\(\"4611686018427387902\"\\",
        "--",
        ">> { ",
        "\"username\" : { \"$",
        "maxKey",
        "\" : 1 } } on : ",
        "shard0001 ",
        "Timestamp\\(2, 5",
        "\\",
        "",
        "33"
      ]
    }
  },
  "Page#34": {
    "Content#0": {
      "##": [
        "Range Sharding",
        "",
        "Ranged sharding involves dividing data into ranges based on the shard ",
        "key values. ",
        "",
        "Each chunk is then assigned a range based on the shard key values.",
        "",
        "",
        "reside on the ",
        "same chunk. ",
        "This allows ",
        "for targeted operations. ",
        "",
        "34"
      ]
    }
  },
  "Page#35": {
    "Content#0": {
      "##": [
        "Shard Key Selection for Ranged Sharding",
        "",
        "Ranged sharding is most efficient when the shard key ",
        "displays the following ",
        "traits. Otherwise, the ",
        "effectiveness of ",
        "horizontal scaling ",
        "will be reduced or ",
        "removed. ",
        "",
        "Large Shard Key ",
        "Cardinality ",
        "",
        "Low ",
        "Shard Key Frequency",
        "",
        "Non",
        "-",
        "Monotonically Changing Shard ",
        "Keys",
        "",
        "35"
      ]
    }
  },
  "Page#36": {
    "Content#0": {
      "##": [
        "Shard key with low cardinality ",
        "",
        "The ",
        "cardinality",
        "of a shard key determines the maximum number of ",
        "chunks the balancer can create",
        ".",
        "",
        "If a shard key has a cardinality of 4, then there can be no more than 4 ",
        "chunks within the ",
        "sharded",
        "cluster. ",
        "",
        "This ",
        "constrains the number of effective shards in the cluster to 4 as ",
        "well ",
        "-",
        "adding additional shards would not provide any benefit.",
        "",
        "36"
      ]
    }
  },
  "Page#37": {
    "Content#0": {
      "##": [
        "Shard key with low cardinality ",
        "",
        "37",
        "The cluster in this example would ",
        "not",
        "scale horizontally",
        ", as ",
        "incoming writes would only route to a subset of shards.",
        "Adding more shards will ",
        "n",
        "ot ",
        "p",
        "rovides any benefit.  "
      ]
    }
  },
  "Page#38": {
    "Content#0": {
      "##": [
        "Shard Key with High Frequency",
        "",
        "The frequency ",
        "of the shard key represents how often a given ",
        "value occurs in the data",
        ".",
        "",
        "If the majority of documents contain only a subset of those ",
        "values, then the chunks storing those documents become a ",
        "bottleneck within the cluster. ",
        "",
        "Furthermore",
        ", as those chunks grow, they may become ",
        "indivisible ",
        "chunks as they cannot be split any further. ",
        "",
        "38"
      ]
    }
  },
  "Page#39": {
    "Content#0": {
      "##": [
        "Shard Key with High Frequency",
        "",
        "39"
      ]
    }
  },
  "Page#40": {
    "Content#0": {
      "##": [
        "Ascending \\(or Descending\\d keys ",
        "The max chunk \\(the chunk with $",
        "maxKey",
        "\\ecomes bottleneck. ",
        "",
        "40"
      ]
    }
  },
  "Page#41": {
    "Content#0": {
      "##": [
        "A shard cluster using a well chosen shard key ",
        "",
        "41"
      ]
    }
  },
  "Page#42": {
    "Content#0": {
      "##": [
        "Tag Sharding",
        "1\\Tag shards",
        "mongos",
        "> ",
        "sh.addShardTag",
        "\\(\"shard0000\", \"US\"\\",
        "WriteResult",
        "\\({ \"",
        "nMatched",
        "\" : 1, \"",
        "nUpserted",
        "\" : 0, \"",
        "nModified",
        "\" : 1 }\\",
        "mongos> ",
        "sh.addShardTag",
        "\\(\"shard0001\", \"EU\"\\",
        "WriteResult",
        "\\({ \"",
        "nMatched",
        "\" : 1, \"",
        "nUpserted",
        "\" : 0, \"",
        "nModified",
        "\" : 1 ",
        "}\\",
        "mongos> ",
        "sh.status",
        "\\(\\",
        "shards:",
        "{  \"_id\" : \"shard",
        "0000",
        "\",  \"host\" : \"127.0.0.1:",
        "27023",
        "\",  \"tags\" : [ ",
        "\"US\" ",
        "",
        "{  \"_id\" : \"shard",
        "0001",
        "\",  \"host\" : \"127.0.0.1:",
        "27024",
        "\",  \"tags\" : [ ",
        "\"EU\" ",
        "",
        "}",
        "2\\nclude the tag in the shard key \\(as the first element\\",
        "mongos",
        "> ",
        "sh.shardCollection",
        "\\(\"",
        "testdb.testtagsharding",
        "\", {",
        "region:1",
        ", ",
        "testkey:1}\\",
        "{ \"",
        "collectionsharded",
        "\" : \"",
        "testdb.testtagsharding",
        "\", \"ok\" : 1 ",
        "}",
        "",
        "42"
      ]
    }
  },
  "Page#43": {
    "Content#0": {
      "##": [
        "3\\ding ",
        "a rule to each shard: ",
        "mongos",
        "> ",
        "sh.addTagRange",
        "\\(\"",
        "testdb.testtagsharding",
        "\", ",
        "{",
        "region:\"US",
        "\"},{region:\"",
        "MaxKey",
        "\"},\"US\"\\",
        "mongos",
        "> ",
        "sh.addTagRange",
        "\\(\"",
        "testdb.testtagsharding",
        "\", ",
        "{",
        "region:MinKey",
        "},{",
        "region:\"US",
        "\"},\"EU\"\\",
        "",
        "43",
        "n",
        "ame space of the collection",
        "m",
        "inimum range \\(inclusive\\",
        "m",
        "aximum range \\(exclusive\\",
        "tag"
      ]
    }
  },
  "Page#44": {
    "Content#0": {
      "##": [
        "",
        "44",
        "m",
        "ongos> ",
        "sh.status",
        "\\(\\",
        "testdb.testtagsharding",
        "shard key: { \"region\" : 1, \"",
        "testkey",
        "\" : 1 }",
        "unique: false",
        "balancing: true",
        "chunks:",
        "shard0000       1",
        "shard0001       2",
        "{ \"",
        "region\" : { \"$",
        "minKey",
        "\" : 1 ",
        "}, \"",
        "testkey",
        "\" : { \"$",
        "minKey",
        "\" : 1 } } ",
        "--",
        ">> { ",
        "\"region\" : ",
        "\"EU\"",
        ", \"",
        "testkey",
        "\" : { \"$",
        "minKey",
        "\" : 1 } } on : shard0001 Timestamp\\(2, 1\\",
        "{ ",
        "\"region\" : \"EU\", ",
        "\"",
        "testkey",
        "\" : { \"$",
        "minKey",
        "\" : 1 } } ",
        "--",
        ">> { ",
        "\"region\" : \"US\", ",
        "\"",
        "testkey",
        "\" : { \"$",
        "minKey",
        "\" : 1 } } on : shard0001 Timestamp\\(1, 3\\",
        "{ ",
        "\"region\" : \"US",
        "\", \"",
        "testkey",
        "\" : { \"$",
        "minKey",
        "\" : 1 } } ",
        "--",
        ">> { ",
        "\"region\" : { \"$",
        "maxKey",
        "\" ",
        ": 1 }, \"",
        "testkey",
        "\" : { \"$",
        "maxKey",
        "\" : 1 } } on : shard0000 Timestamp\\(2, 0\\",
        "tag: EU  { \"region\" : { \"$",
        "minKey",
        "\" : 1 } } ",
        "--",
        ">> { \"region\" : \"US\" }",
        "tag: US  { \"region\" : \"US\" } ",
        "--",
        ">> { \"region\" : \"",
        "MaxKey",
        "\" }"
      ]
    }
  },
  "Page#45": {
    "Content#0": {
      "##": [
        "Populate data ",
        "mongos> for \\(",
        "i",
        "=0; ",
        "i",
        "< 10000; ",
        "i",
        "++\\",
        "{",
        "db.getSiblingDB",
        "\\(\"",
        "testdb",
        "\"\\",
        "testtagsharding.insert",
        "\\({",
        "region:\"EU",
        "\", ",
        "testkey:i",
        "}\\",
        "WriteResult",
        "\\({ \"",
        "nInserted",
        "\" : 1 }\\",
        "mongos>",
        "mongos> ",
        "db.testtagsharding.count",
        "\\(\\",
        "10000",
        "",
        "45"
      ]
    }
  },
  "Page#46": {
    "Content#0": {
      "##": [
        "",
        "vagrant@vagrant",
        "-",
        "ubuntu",
        "-",
        "trusty",
        "-",
        "64",
        ":~$ ",
        "sudo",
        "mongo 127.0.0.1:27023",
        "> ",
        "use ",
        "testdb",
        "switched to ",
        "db",
        "testdb",
        "> ",
        "db.testtagsharding.count",
        "\\(\\",
        "0",
        "",
        "vagrant@vagrant",
        "-",
        "ubuntu",
        "-",
        "trusty",
        "-",
        "64",
        ":~$ ",
        "sudo",
        "mongo 127.0.0.1:27024",
        "> ",
        "use ",
        "testdb",
        "switched to ",
        "db",
        "testdb",
        "> ",
        "db.testtagsharding.count",
        "\\(\\",
        "10000",
        "",
        "46"
      ]
    }
  },
  "Page#47": {
    "Content#0": {
      "##": [
        "Replication vs. Sharding",
        "",
        "Replication creates an exact copy of data on multiple servers, ",
        "so every server in a replica set is a mirror image of every ",
        "other server. ",
        "",
        "Every shard contains a different subset of data. ",
        "",
        "47"
      ]
    }
  }
}